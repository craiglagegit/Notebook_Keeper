{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Beam Simulator Images and Brighter-fatter Correction\n",
    "<br>Owner(s): **Andrew Bradshaw** ([@andrewkbradshaw](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@andrewkbradshaw))\n",
    "<br>Last Verified to Run: **2018-08-10**\n",
    "<br>Verified Stack Release: **16.0 and 16.0+22 (w_2018_31)**\n",
    "\n",
    "This notebook demonstrates the [brighter-fatter systematic error](https://arxiv.org/abs/1402.0725) on images of stars and galaxies illuminated on an ITL-3800C-002 CCD at the [UC Davis LSST beam simulator laboratory](https://arxiv.org/abs/1411.5667). Using a series of images at increasing exposure times, we demonstrate the broadening of image profiles on DM stack shape measurements, and a [possible correction method](https://arxiv.org/abs/1711.06273) which iteratively applies a kernel to restore electrons to the pixels which they were deflected from. To keep things simple, for now we skip DM stack instrument signature removal (ISR) and work on a subset of images which are arrays (500x500) of electron counts in pixels.\n",
    "\n",
    "### Learning Objectives:\n",
    "\n",
    "After working through this tutorial you should be able to: \n",
    "1. Characterize and measure objects (stars/galaxies) in LSST beam simulator images\n",
    "2. Test the Brighter-Fatter kernel correction method on those images\n",
    "3. Build your own tests of stack instrument signature removal algorithms\n",
    "\n",
    "### Logistics\n",
    "This notebook is intended to be runnable on `lsst-lspdev.ncsa.illinois.edu` from a local git clone of https://github.com/LSSTScienceCollaborations/StackClub.\n",
    "\n",
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What version of the Stack am I using?\n",
    "! echo $HOSTNAME\n",
    "! eups list -s | grep lsst_distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from astropy.io import fits\n",
    "import time,glob\n",
    "\n",
    "# if running stack v16.0, silence a long matplotlib Agg warning with:\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in an image, then set the variance plane based upon it\n",
    "Cut-outs of beam simulator star/galaxy images have been placed in the shared data directory at `/project/shared/data/beamsim/bfcorr/`. We skip (for now) most of the instrument signature removal (ISR) steps because these are preprocessed images (bias subtracted, gain corrected). We instead start by reading in one of those `.fits` files and making an image plane `afwImage.ExposureF` as well as a variance plane, which is then ready for characterization and calibration in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lsst.afw.image as afwImage\n",
    "from lsst.ip.isr.isrFunctions import updateVariance\n",
    "\n",
    "# where the data lives, choosing one image to start\n",
    "fitsglob='/project/shared/data/beamsim/bfcorr/*part.fits'\n",
    "fitsfilename = np.sort(glob.glob(fitsglob))[19]  \n",
    "\n",
    "# Read in a single image to an afwImage.ImageF object\n",
    "image_array=afwImage.ImageF.readFits(fitsfilename)\n",
    "image = afwImage.ImageF(image_array)\n",
    "exposure = afwImage.ExposureF(image.getBBox())\n",
    "exposure.setImage(image)\n",
    "hdr=fits.getheader(fitsfilename) # the header has some useful info in it\n",
    "print(\"Read in \",fitsfilename.split('/')[-1])\n",
    "\n",
    "# Set the variance plane using the image plane via updateVariance function\n",
    "gain = 1.0 # because these images are already gain corrected\n",
    "readNoise = 10.0  # in electrons\n",
    "updateVariance(exposure.getMaskedImage(), gain, readNoise)\n",
    "\n",
    "# Another way of setting variance and/or masks?\n",
    "#mask = afwImage.makeMaskFromArray(np.zeros((4000,4072)).astype('int32'))\n",
    "#variance = afwImage.makeImageFromArray((readNoise**2 + image_array.array())\n",
    "#masked_image = afwImage.MaskedImageF(image, mask, variance)\n",
    "#exposure = afwImage.ExposureF(masked_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the image and its electron distribution\n",
    "plt.figure(figsize=(12,5)),plt.subplots_adjust(wspace=.3)\n",
    "plt.suptitle('Star/galaxy beam sim segment of '+hdr['CCD_SERN']+'\\n '+fitsfilename.split('/')[-1])\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(exposure.getImage().array,vmax=1e3,origin='lower')\n",
    "plt.colorbar(label='electrons')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(exposure.getImage().array.flatten(),bins=1000,histtype='step')\n",
    "plt.yscale('log')#,plt.xscale('log')\n",
    "plt.xlabel('Number of electrons in pixel'),plt.ylabel('Number of pixels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +TODO perhaps some other image stats from \n",
    "# https://github.com/lsst/pipe_tasks/blob/master/python/lsst/pipe/tasks/exampleStatsTasks.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform image characterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.pipe.tasks.characterizeImage import CharacterizeImageTask, CharacterizeImageConfig\n",
    "\n",
    "# first set a few configs that are specific to the data\n",
    "charConfig = CharacterizeImageConfig()\n",
    "#this set the fwhm of the simple PSF to that of the fwhm used in the simulation\n",
    "charConfig.installSimplePsf.fwhm = .2\n",
    "charConfig.doMeasurePsf = False\n",
    "charConfig.doApCorr = False\n",
    "charConfig.repair.doCosmicRay = False  \n",
    "# we do have some cosmic rays, but we also subpixel features and an undersampled PSF\n",
    "charConfig.detection.background.binSize = 10 \n",
    "#charConfig.background.binSize = 50\n",
    "charConfig.detection.minPixels = 5\n",
    "charTask = CharacterizeImageTask(config=charConfig)\n",
    "\n",
    "# charTask.run?  # works for v16.0+22\n",
    "charTask.characterize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstart=time.time()\n",
    "charResult = charTask.characterize(exposure) # charTask.run(exposure) stack v16.0+22\n",
    "print(\"Characterization took \",str(time.time()-tstart)[:4],\" seconds\")\n",
    "print(\"Detected \",len(charResult.sourceCat),\" objects \")\n",
    "\n",
    "plt.title('X/Y locations of detections')\n",
    "plt.plot(charResult.sourceCat['base_SdssCentroid_x'],charResult.sourceCat['base_SdssCentroid_y'],'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the mask plane, which started off as all zeros\n",
    "# and now has some values of 2^5\n",
    "maskfoo=exposure.getMask()\n",
    "print(\"Unique mask plane values: \",np.unique(maskfoo.array))\n",
    "print(\"Mask dictionary entries: \",maskfoo.getMaskPlaneDict())\n",
    "\n",
    "plt.figure(figsize=(12,5)),plt.subplots_adjust(wspace=.3)\n",
    "plt.subplot(121)\n",
    "plt.imshow(maskfoo.array,origin='lower'),plt.colorbar()\n",
    "plt.subplot(122)\n",
    "plt.hist(maskfoo.array.flatten()),plt.xlabel('Mask plane values')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Another option for this analysis, from:\n",
    "# https://gist.github.com/josePhoenix/8325c16b44fb5fa51f40261b184a78ef\n",
    "\n",
    "import lsst.afw.table\n",
    "import lsst.afw.image\n",
    "import lsst.afw.math\n",
    "import lsst.meas.algorithms\n",
    "import lsst.meas.base\n",
    "import lsst.meas.deblender\n",
    "\n",
    "schema = lsst.afw.table.SourceTable.makeMinimalSchema()\n",
    "detect = lsst.meas.algorithms.SourceDetectionTask(schema=schema)\n",
    "deblend = lsst.meas.deblender.SourceDeblendTask(schema=schema)\n",
    "measure = lsst.meas.base.SingleFrameMeasurementTask(schema=schema)\n",
    "\n",
    "tstart=time.time()\n",
    "\n",
    "table = lsst.afw.table.SourceTable.make(schema)  # this is really just a factory for records, not a table\n",
    "\n",
    "detect_result = detect.run(table, exposure)\n",
    "catalog = detect_result.sources   # this is the actual catalog, but most of it's still empty\n",
    "print(time.time()-tstart)\n",
    "\n",
    "#deblend.run(exposure, catalog)\n",
    "#print(time.time()-tstart)\n",
    "\n",
    "measure.run(catalog, exposure)\n",
    "print(time.time()-tstart)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(catalog['base_SdssCentroid_x'],catalog['base_SdssCentroid_y'],marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform further image calibration and measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to do astrometry or photometry calibration\n",
    "#since this is a lab image\n",
    "from lsst.pipe.tasks.calibrate import CalibrateTask, CalibrateConfig\n",
    "calConfig = CalibrateConfig()\n",
    "calConfig.doAstrometry = False\n",
    "calConfig.doPhotoCal = False\n",
    "calConfig.detection.minPixels = 15\n",
    "calConfig.doApCorr = False\n",
    "calConfig.doDeblend = False   # these are well-separated objects, deblending adds time & trouble\n",
    "calConfig.detection.background.binSize = 50\n",
    "calTask = CalibrateTask(config= calConfig, icSourceSchema=charResult.sourceCat.schema)\n",
    "\n",
    "#calTask.run? # for stack v16.0+22 \n",
    "calTask.calibrate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstart=time.time()\n",
    "# for stack v16.0+22, change to calTask.run(charResult.exposure)\n",
    "calResult = calTask.calibrate(charResult.exposure, background=charResult.background,\n",
    "                              icSourceCat = charResult.sourceCat)\n",
    "\n",
    "print(\"Calibration took \",str(time.time()-tstart)[:4],\" seconds\")\n",
    "print(\"Detected \",len(calResult.sourceCat),\" objects \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the source catalog which has now been attached to the \n",
    "src=calResult.sourceCat  #.copy(deep=True) ?\n",
    "#print(src.asAstropy)\n",
    "\n",
    "src.writeFits(fitsfilename+'.cat')\n",
    "# read back in and access via:\n",
    "#catalog=fits.open(fitsfilename+'.cat')\n",
    "#catalog[1].data['base_SdssShape_xx'] etc.\n",
    "\n",
    "plt.figure()\n",
    "par_names=['base_SdssShape_xx','base_SdssShape_yy','base_SdssShape_flux']\n",
    "par_mins=[0,0,0]\n",
    "par_maxs=[5,5,1e6]\n",
    "n_par=len(par_names)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5*n_par,6)),plt.subplots_adjust(wspace=.25)\n",
    "for par_name,par_min,par_max,i in zip(par_names,par_mins,par_maxs,range(n_par)):\n",
    "    plt.subplot(2,n_par,i+1)\n",
    "    plt.scatter(src['base_SdssCentroid_x'],src['base_SdssCentroid_y'],c=src[par_name],marker='o',vmin=par_min,vmax=par_max)\n",
    "    plt.xlabel('X'),plt.ylabel('Y'),plt.colorbar(label=par_name)\n",
    "\n",
    "\n",
    "    plt.subplot(2,n_par,n_par+i+1)\n",
    "    plt.hist(src[par_name],range=[par_min,par_max],bins=20,histtype='step')\n",
    "    plt.xlabel(par_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the image with Firefly and overlay detected objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lsst.afw.display as afwDisplay\n",
    "\n",
    "# Firefly client imports\n",
    "from firefly_client import FireflyClient\n",
    "\n",
    "# Standard libraries in support of Firefly display\n",
    "from urllib.parse import urlparse, urlunparse, ParseResult\n",
    "from IPython.display import IFrame, display, Markdown\n",
    "import os\n",
    "\n",
    "# Own cell?\n",
    "my_channel = '{}_test_channel'.format(os.environ['USER'])\n",
    "server = 'https://lsst-lspdev.ncsa.illinois.edu'\n",
    "\n",
    "# This needs its own cell\n",
    "ff='{}/firefly/slate.html?__wsch={}'.format(server, my_channel)\n",
    "IFrame(ff,1000,600)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the backend and attach to the waiting display channel\n",
    "afwDisplay.setDefaultBackend('firefly')\n",
    "afw_display = afwDisplay.getDisplay(frame=1, \n",
    "                                    name=my_channel)\n",
    "\n",
    "# Open the exposure (Firefly knows about mask planes)\n",
    "afw_display.mtv(exposure)\n",
    "\n",
    "#Now we’ll overplot sources from the src table onto the image display using the Display’s dot method for plotting markers. \n",
    "#Display.dot plots markers individually, so you’ll need to iterate over rows in the SourceTable. \n",
    "#Next we display the first 100 sources. We limit the number of sources since plotting the whole catalog \n",
    "#is a serial process and takes some time. Because of this, it is more efficient to send a batch of updates to the display, \n",
    "#so we enclose the loop in a display.Buffering context, like this:\n",
    "\n",
    "afw_display.erase()\n",
    "\n",
    "with afw_display.Buffering():\n",
    "    for record in src[:]:\n",
    "        afw_display.dot('o', record.getX(), record.getY(), size=20, ctype='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the brighter-fatter correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.ip.isr.isrTask import IsrTask # brighterFatterCorrection lives here\n",
    "isr=IsrTask()\n",
    "\n",
    "pre_bfcorr_exposure=exposure.clone() #save a copy of the pre-bf corrected image\n",
    "\n",
    "isr.brighterFatterCorrection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the kernel (determined from e.g. simulations or flat fields)\n",
    "kernel=fits.getdata('/project/shared/data/beamsim/bfcorr/BF_kernel-ITL_3800C_002.fits')\n",
    "\n",
    "# Perform the correction\n",
    "tstart=time.time()\n",
    "exposure=pre_bfcorr_exposure.clone()\n",
    "isr.brighterFatterCorrection(exposure,kernel,20,10,False)\n",
    "print(\"Brighter-fatter correction took\",time.time()-tstart,\" seconds\") #takes 99 seconds for 4kx4k exposure, 21x21 kernel, 20 iterations, 10 thresh\n",
    "\n",
    "# Plot kernel and image differences\n",
    "plt.figure(),plt.title('BF kernel')\n",
    "plt.imshow(kernel),plt.colorbar()\n",
    "\n",
    "imagediff=(pre_bfcorr_exposure.getImage().array-exposure.getImage().array)\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.subplot(231),plt.title('Before')\n",
    "plt.imshow(pre_bfcorr_exposure.getImage().array,vmin=0,vmax=1e3,origin='lower'),plt.colorbar()\n",
    "plt.subplot(232),plt.title('After')\n",
    "plt.imshow(exposure.getImage().array,vmin=0,vmax=1e3,origin='lower'),plt.colorbar()\n",
    "plt.subplot(233),plt.title('Before - After')\n",
    "vmin,vmax=-50,50\n",
    "plt.imshow(imagediff,vmin=vmin,vmax=vmax,origin='lower'),plt.colorbar()\n",
    "\n",
    "nbins=1000\n",
    "plt.subplot(234)\n",
    "plt.hist(pre_bfcorr_exposure.getImage().array.flatten(),bins=nbins,histtype='step',label='before'),plt.yscale('log')\n",
    "plt.subplot(235)\n",
    "plt.hist(exposure.getImage().array.flatten(),bins=nbins,histtype='step',label='after'),plt.yscale('log')\n",
    "plt.subplot(236)\n",
    "plt.hist(imagediff.flatten(),bins=nbins,histtype='step',label='difference'),plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel('Pixel values [e-]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the characterization & measurement over the 20 exposures of increasing brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell runs through all of the image parts, and should take around 1 second per image (20 sec total)\n",
    "\n",
    "do_bf_corr=False  # True or False, makes catalogs (.cat) for all of the corrected and uncorrected images\n",
    "fitsglob='/project/shared/data/beamsim/bfcorr/*part.fits'\n",
    "for fitsfilename in np.sort(glob.glob(fitsglob)):\n",
    "    image_array=afwImage.ImageF.readFits(fitsfilename)\n",
    "    image = afwImage.ImageF(image_array)\n",
    "\n",
    "    exposure = afwImage.ExposureF(image.getBBox())\n",
    "    exposure.setImage(image)\n",
    "\n",
    "    updateVariance(exposure.getMaskedImage(), gain, readNoise)\n",
    "    \n",
    "    # start the characterization and measurement, optionally beginning with the brighter-fatter correction\n",
    "    tstart=time.time()\n",
    "    if do_bf_corr:\n",
    "        isr.brighterFatterCorrection(exposure,kernel,20,10,False)\n",
    "        #print(\"Brighter-fatter correction took\",str(time.time()-tstart)[:4],\" seconds\")\n",
    "    # for stack v16.0+22 use charTask.run() and calTask.run()\n",
    "    charResult = charTask.characterize(exposure)  \n",
    "    calResult = calTask.calibrate(charResult.exposure, background=charResult.background,\n",
    "                                  icSourceCat = charResult.sourceCat)\n",
    "    src=calResult.sourceCat  #.copy(deep=True) ?\n",
    "    \n",
    "    # write out the source catalog\n",
    "    catfilename=fitsfilename.replace('.fits','.cat')\n",
    "    if do_bf_corr: catfilename=catfilename.replace('.cat','-bfcorr.cat')\n",
    "    src.writeFits(catfilename)\n",
    "    \n",
    "    print(fitsfilename.split('/')[-1],\" char. & calib. took \",str(time.time()-tstart)[:4],\" seconds to measure \",len(calResult.sourceCat),\" objects \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some of the source catalog shape measurements\n",
    "for name in src.schema.getOrderedNames():\n",
    "    if 'shape' in name.lower():\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the catalogs, both corrected and uncorrected (this could be improved with pandas)\n",
    "cat_arr = []\n",
    "catglob='/project/shared/data/beamsim/bfcorr/ITL*part.cat' # uncorrected catalogs\n",
    "for catfilename in np.sort(glob.glob(catglob)): cat_arr.append(fits.getdata(catfilename))\n",
    "\n",
    "bf_cat_arr = []\n",
    "catglob='/project/shared/data/beamsim/bfcorr/ITL*part-bfcorr.cat' # corrected catalogs\n",
    "for catfilename in np.sort(glob.glob(catglob)): bf_cat_arr.append(fits.getdata(catfilename))\n",
    "ncats=len(cat_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show possible issues with source matching which we will remedy with simple matching in the next cell\n",
    "for i in range(ncats):\n",
    "    xfoo,yfoo=cat_arr[i]['base_SdssCentroid_x'],cat_arr[i]['base_SdssCentroid_y']\n",
    "    plt.plot(xfoo,yfoo,'o')\n",
    "plt.title('Centroids of sequential exposures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a fiducial frame as reference, we simply match the catalogs by looking for single objects\n",
    "# within a max distance\n",
    "\n",
    "fidframe=10\n",
    "maxdist=.5\n",
    "\n",
    "x0s,y0s=cat_arr[fidframe]['base_SdssCentroid_x'],cat_arr[fidframe]['base_SdssCentroid_y']\n",
    "nspots=len(x0s)\n",
    "bf_dat=np.empty((ncats,nspots,6)) #x,y,xx,yy,bfxx,bfyy\n",
    "bf_dat[:]=np.nan\n",
    "\n",
    "for i in range(ncats):\n",
    "    # get the centroids of objects in the bf-corrected and uncorrected images\n",
    "    x1,y1=cat_arr[i]['base_SdssCentroid_x'],cat_arr[i]['base_SdssCentroid_y']\n",
    "    x1_bf,y1_bf=bf_cat_arr[i]['base_SdssCentroid_x'],bf_cat_arr[i]['base_SdssCentroid_y']\n",
    "    for j in range(nspots):   # loop over fiducial frame centroids to find matches\n",
    "        x0,y0=x0s[j],y0s[j]\n",
    "        # find the matches between the fiducial centroid (x0,y0) and the corrected/uncorrected ones\n",
    "        bf_gd=np.where(np.sqrt((x1_bf-x0)**2+(y1_bf-y0)**2)<maxdist)[0]\n",
    "        gd=np.where(np.sqrt((x1-x0)**2+(y1-y0)**2)<maxdist)[0]\n",
    "        if (len(bf_gd)==1 & len(gd)==1):  # only take single matches\n",
    "            xx,yy=cat_arr[i]['base_SdssShape_xx'][gd],cat_arr[i]['base_SdssShape_yy'][gd]\n",
    "            xx_bf,yy_bf=bf_cat_arr[i]['base_SdssShape_xx'][bf_gd],bf_cat_arr[i]['base_SdssShape_yy'][bf_gd]\n",
    "            bf_dat[i,j,:]=x0,y0,xx,yy,xx_bf,yy_bf  # keep those above measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the brighter-fatter effect on those shape measurements and the corrected version\n",
    "# These are good indices to look with defaults: [0,1,6,12,23,35,44,46,52,56,59,69,71,73,90]\n",
    "\n",
    "# +TODO - change the exposure number to some brightness measurement\n",
    "# +TODO - get postage stamps in a stackly manner\n",
    "\n",
    "nfoo=44\n",
    "plt.figure(figsize=(14,4)),plt.subplots_adjust(wspace=.3)\n",
    "# grab a postage stamp, +TODO in a stackly manner\n",
    "sz=11\n",
    "xc,yc=bf_dat[10,nfoo,0].astype('int')+1,bf_dat[10,nfoo,1].astype('int')+1\n",
    "stamp=exposure.getImage().array[yc-sz:yc+sz,xc-sz:xc+sz]\n",
    "plt.subplot(131)\n",
    "plt.imshow(stamp,origin='lower',norm=LogNorm(1,stamp.max())),plt.colorbar()\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(bf_dat[:,nfoo,2],'r.',label='Uncorrected')\n",
    "plt.plot(bf_dat[:,nfoo,4],'g.',label='Corrected')\n",
    "plt.xlabel('Exposure number'),plt.ylabel('base_SdssShape_xx')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(bf_dat[:,nfoo,3],'r.',label='Uncorrected')\n",
    "plt.plot(bf_dat[:,nfoo,5],'g.',label='Corrected')\n",
    "plt.xlabel('Exposure number'),plt.ylabel('base_SdssShape_yy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +TODO add re-scaled stamp subtraction comparison\n",
    "# +TODO other ways of doing matching, catalog stacking\n",
    "# should this analysis focus on only one stamp? realism in wide-field correction, simplicity in stamp by stamp..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
