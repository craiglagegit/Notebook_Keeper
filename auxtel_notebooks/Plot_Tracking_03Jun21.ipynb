{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbe35aef",
   "metadata": {},
   "source": [
    "## AuxTel Mount fails - 03-Jun-21\n",
    "\n",
    "In this notebook, investigate mount tracking on 25-May-21\\\n",
    "I can get things to line up, by repeatedly telling the code that UTC times are really TAI. \\\n",
    "This is all a big mess, but this seems to work for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd1f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time, os, asyncio\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from astropy.time import Time, TimeDelta\n",
    "from lsst_efd_client import EfdClient\n",
    "from lsst.daf.persistence import Butler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0feb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get EFD client and bring in Lupton's unpacking code\n",
    "client = EfdClient('ldf_stable_efd')\n",
    "#client = EfdClient('summit_efd')\n",
    "\n",
    "def merge_packed_time_series(packed_dataframe, base_field, stride=1, \n",
    "                             ref_timestamp_col=\"cRIO_timestamp\", internal_time_scale=\"tai\"): \n",
    "    \"\"\"Select fields that are time samples and unpack them into a dataframe.\n",
    "            Parameters\n",
    "            ----------\n",
    "            packedDF : `pandas.DataFrame`\n",
    "                packed data frame containing the desired data\n",
    "            base_field :  `str`\n",
    "                Base field name that will be expanded to query all\n",
    "                vector entries.\n",
    "            stride : `int`, optional\n",
    "                Only use every stride value when unpacking.  Must be a factor\n",
    "                of the number of packed values.\n",
    "                (1 by default)\n",
    "            ref_timestamp_col : `str`, optional\n",
    "                Name of the field name to use to assign timestamps to unpacked\n",
    "                vector fields (default is 'cRIO_timestamp').\n",
    "            internal_time_scale : `str`, optional\n",
    "                Time scale to use when converting times to internal formats\n",
    "                ('tai' by default). Equivalent to EfdClient.internal_scale\n",
    "        Returns\n",
    "            -------\n",
    "            result : `pandas.DataFrame`\n",
    "                A `pandas.DataFrame` containing the results of the query.\n",
    "            \"\"\"\n",
    "    \n",
    "    packed_fields = [k for k in packed_dataframe.keys() if k.startswith(base_field)]\n",
    "    packed_fields = sorted(packed_fields, key=lambda k: int(k[len(base_field):]))  # sort by pack ID\n",
    "    npack = len(packed_fields)\n",
    "    if npack%stride != 0:\n",
    "        raise RuntimeError(f\"Stride must be a factor of the number of packed fields: {stride} v. {npack}\")\n",
    "    packed_len = len(packed_dataframe)\n",
    "    n_used = npack//stride   # number of raw fields being used\n",
    "    output = np.empty(n_used*packed_len)\n",
    "    times = np.empty_like(output, dtype=packed_dataframe[ref_timestamp_col][0])\n",
    "    \n",
    "    if packed_len == 1:\n",
    "        dt = 0\n",
    "    else:\n",
    "        dt = (packed_dataframe[ref_timestamp_col][1] - packed_dataframe[ref_timestamp_col][0])/npack\n",
    "    for i in range(0, npack, stride):\n",
    "        i0 = i//stride\n",
    "        output[i0::n_used] = packed_dataframe[f\"{base_field}{i}\"]\n",
    "        times[i0::n_used] = packed_dataframe[ref_timestamp_col] + i*dt\n",
    "     \n",
    "    timestamps = Time(times, format='unix', scale=internal_time_scale).datetime64\n",
    "    return pd.DataFrame({base_field:output, \"times\":times}, index=timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dbdca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still using Gen2 for this simple task\n",
    "REPO_DIR = '/project/shared/auxTel'\n",
    "butler = Butler(REPO_DIR)\n",
    "dayObs = '2021-05-25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e347aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These two exposures are of two different objects, so we know we slewed between them\n",
    "# Note that the DATE keyword is ~ 30 seconds before DATE-BEG and DATE-END\n",
    "# This doesn't seem right\n",
    "expIds = [2021052500183, 2021052500184]\n",
    "mDatas = {}\n",
    "for expId in expIds:\n",
    "    exp = butler.get('raw', detector=0, expId=expId)\n",
    "    mData = exp.getMetadata()\n",
    "    print(f\"{expId} \\t {mData['OBJECT']} \\t {mData['DATE']} \\t {mData['DATE-BEG']} \\t {mData['DATE-END']}\")\n",
    "    mDatas[expId] = mData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cb7d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these for finding the \"allAxesInPosition\" timestamp\n",
    "# The inPosition timestamp makes sense with the DATE-BEG and DATE-END times\n",
    "# But are these times in UTC, or TAI?\n",
    "# The cells below indicate they are in UTC, but here I am forcing them to be in TAI.\n",
    "start = Time(mDatas[expIds[0]]['DATE-BEG'],format='isot', scale='utc')\n",
    "end = Time(mDatas[expIds[1]]['DATE-BEG'],format='isot', scale='utc')\n",
    "timestamp = f\"time >= '{start}+00:00' AND time <= '{end}+00:00'\"\n",
    "query = f'SELECT \"inPosition\" FROM \"efd\".\"autogen\".\"lsst.sal.ATMCS.logevent_allAxesInPosition\"\\\n",
    "    WHERE {timestamp} and inPosition = true'\n",
    "\n",
    "inPosition = await client.influx_client.query(query)\n",
    "print(inPosition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe63640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why is the timestamp in UTC?\n",
    "inPosition.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce33e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Time(inPosition.index[0]).utc, Time(inPosition.index[0]).tai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fac6509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the mount tracking info for a time before and after the inPosition timestamp.\n",
    "before = 60.0\n",
    "after = 60.0\n",
    "inPos = Time(inPosition.index[0], scale='tai') # We lie to it and tell it it is TAI.\n",
    "tstart = inPos - TimeDelta(before, format='sec')\n",
    "tend = inPos + TimeDelta(after, format='sec')\n",
    "print(f\"{inPos} \\t {tstart} \\t {tend}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f6fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and plot the data\n",
    "# Note that when it gets the data, it adds another 37 seconds to tstart and tend!!!\n",
    "# If I change merge_packed_time_series internal_time_scale to 'utc', then it doesn't do this.\n",
    "mount_position = await client.select_time_series(\"lsst.sal.ATMCS.mount_AzEl_Encoders\", ['*'],\n",
    "                                          tstart, tend)\n",
    "nasmyth_position = await client.select_time_series(\"lsst.sal.ATMCS.mount_Nasmyth_Encoders\", ['*'],\n",
    "                                          tstart, tend)\n",
    "\n",
    "az = merge_packed_time_series(mount_position, 'azimuthCalculatedAngle', stride=1, internal_time_scale=\"utc\")\n",
    "el = merge_packed_time_series(mount_position, 'elevationCalculatedAngle', stride=1, internal_time_scale=\"utc\")\n",
    "rot = merge_packed_time_series(nasmyth_position, 'nasmyth2CalculatedAngle', stride=1, internal_time_scale=\"utc\")\n",
    "\n",
    "# Plot it\n",
    "fig = plt.figure(figsize = (16,6))\n",
    "plt.suptitle(f\"Mount Tracking - ExpId {expIds[1]}\", fontsize = 18)\n",
    "# Azimuth axis\n",
    "plt.subplot(1,3,1)\n",
    "ax1 = az['azimuthCalculatedAngle'].plot(legend=True, color='red')\n",
    "ax1.set_title(\"Azimuth axis\", fontsize=16)\n",
    "ax1.axvline(inPos.isot, color=\"green\", linestyle=\"--\", label=\"In Position\")\n",
    "ax1.set_ylabel(\"Degrees\")\n",
    "ax1.legend()\n",
    "\n",
    "# Elevation axis\n",
    "plt.subplot(1,3,2)\n",
    "ax2 = el['elevationCalculatedAngle'].plot(legend=True, color='green')\n",
    "ax2.set_title(\"Elevation axis\", fontsize=16)\n",
    "ax2.axvline(inPos.isot, color=\"green\", linestyle=\"--\", label=\"In Position\")\n",
    "ax2.legend()\n",
    "\n",
    "# Nasmyth2 rotator axis\n",
    "plt.subplot(1,3,3)\n",
    "ax3 = rot['nasmyth2CalculatedAngle'].plot(legend=True, color='blue')\n",
    "ax3.set_title(\"Nasmyth2 axis\", fontsize=16)\n",
    "ax3.axvline(inPos.isot, color=\"green\", linestyle=\"--\", label=\"In Position\")\n",
    "ax3.legend()\n",
    "\n",
    "plt.savefig(f\"/project/cslage/AuxTel/offsets/Tracking_Timebase_{expIds[1]}_25May21.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d2b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
