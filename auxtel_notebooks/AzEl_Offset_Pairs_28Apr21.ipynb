{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AuxTel AzEl offsets - 28-Apr-21\n",
    "\n",
    "In this notebook, investigate az-el offsets from 11-Mar-21\\\n",
    "This is going to analyze a pair of exposures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time, os, asyncio, glob\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import astropy.io.fits as pf\n",
    "from astropy.time import Time, TimeDelta\n",
    "from astropy.time import Time, TimeDelta\n",
    "from astropy.coordinates import SkyCoord, AltAz, ICRS, EarthLocation, Angle, FK5\n",
    "import astropy.units as u\n",
    "\n",
    "from lsst.daf.butler import Butler as gen3Butler\n",
    "from lsst.daf.persistence import Butler as gen2Butler\n",
    "from lsst_efd_client import EfdClient\n",
    "from lsst.pipe.tasks.characterizeImage import CharacterizeImageTask, CharacterizeImageConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get EFD client\n",
    "client = EfdClient('summit_efd')\n",
    "\n",
    "# This is Lupton's code for unpacking the 100/second data\n",
    "def merge_packed_time_series(packed_dataframe, base_field, stride=1, \n",
    "                             ref_timestamp_col=\"cRIO_timestamp\", internal_time_scale=\"tai\"):\n",
    "    \"\"\"Select fields that are time samples and unpack them into a dataframe.\n",
    "            Parameters\n",
    "            ----------\n",
    "            packedDF : `pandas.DataFrame`\n",
    "                packed data frame containing the desired data\n",
    "            base_field :  `str`\n",
    "                Base field name that will be expanded to query all\n",
    "                vector entries.\n",
    "            stride : `int`, optional\n",
    "                Only use every stride value when unpacking.  Must be a factor\n",
    "                of the number of packed values.\n",
    "                (1 by default)\n",
    "            ref_timestamp_col : `str`, optional\n",
    "                Name of the field name to use to assign timestamps to unpacked\n",
    "                vector fields (default is 'cRIO_timestamp').\n",
    "            internal_time_scale : `str`, optional\n",
    "                Time scale to use when converting times to internal formats\n",
    "                ('tai' by default). Equivalent to EfdClient.internal_scale\n",
    "        Returns\n",
    "            -------\n",
    "            result : `pandas.DataFrame`\n",
    "                A `pandas.DataFrame` containing the results of the query.\n",
    "            \"\"\"\n",
    "    \n",
    "    packed_fields = [k for k in packed_dataframe.keys() if k.startswith(base_field)]\n",
    "    packed_fields = sorted(packed_fields, key=lambda k: int(k[len(base_field):]))  # sort by pack ID\n",
    "    npack = len(packed_fields)\n",
    "    if npack%stride != 0:\n",
    "        raise RuntimeError(f\"Stride must be a factor of the number of packed fields: {stride} v. {npack}\")\n",
    "    packed_len = len(packed_dataframe)\n",
    "    n_used = npack//stride   # number of raw fields being used\n",
    "    output = np.empty(n_used*packed_len)\n",
    "    times = np.empty_like(output, dtype=packed_dataframe[ref_timestamp_col][0])\n",
    "    \n",
    "    if packed_len == 1:\n",
    "        dt = 0\n",
    "    else:\n",
    "        dt = (packed_dataframe[ref_timestamp_col][1] - packed_dataframe[ref_timestamp_col][0])/npack\n",
    "    for i in range(0, npack, stride):\n",
    "        i0 = i//stride\n",
    "        output[i0::n_used] = packed_dataframe[f\"{base_field}{i}\"]\n",
    "        times[i0::n_used] = packed_dataframe[ref_timestamp_col] + i*dt\n",
    "     \n",
    "    timestamps = Time(times, format='unix', scale=internal_time_scale).datetime64\n",
    "    return pd.DataFrame({base_field:output, \"times\":times}, index=timestamps)\n",
    "\n",
    "# This makes the plots more readable\n",
    "def colorbar(mappable):\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    last_axes = plt.gca()\n",
    "    ax = mappable.axes\n",
    "    fig = ax.figure\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = fig.colorbar(mappable, cax=cax)\n",
    "    plt.sca(last_axes)\n",
    "    return cbar\n",
    "\n",
    "# Set up charImage task\n",
    "charConfig = CharacterizeImageConfig()\n",
    "charConfig.doMeasurePsf = False#True\n",
    "charConfig.doApCorr = False\n",
    "charConfig.doDeblend = False\n",
    "charConfig.repair.doCosmicRay = True\n",
    "charConfig.repair.doInterpolate = True   \n",
    "charConfig.detection.minPixels = 500\n",
    "charTask = CharacterizeImageTask(config=charConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gen3 butler\n",
    "dayObs = 20210311\n",
    "REPO_DIR = '/repo/main'\n",
    "butler = gen3Butler(REPO_DIR, collections=\"LATISS/raw/all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "expId1 = 2021031100418\n",
    "expId2 = 2021031100419\n",
    "mData1 = butler.get('raw.metadata', detector=0, exposure=expId1)\n",
    "exp1Start = Time(mData1['DATE-BEG'],scale='tai')\n",
    "exp1End = Time(mData1['DATE-END'],scale='tai')\n",
    "mData2 = butler.get('raw.metadata', detector=0, exposure=expId2)\n",
    "exp2Start = Time(mData2['DATE-BEG'],scale='tai')\n",
    "exp2End = Time(mData2['DATE-END'],scale='tai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are for finding the timestamps of the offset event\n",
    "offsets = await client.select_time_series(\"lsst.sal.ATPtg.command_offsetAzEl\", ['*'],\n",
    "                                          exp1Start, exp2End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the programmed offsets.\n",
    "print(f\"Number of offset commands = {len(offsets)}.  It should be just one.\")\n",
    "for i, offset in enumerate(offsets.values):\n",
    "    print(i, Time(offsets.index[i]).tai.isot,offset[0], offset[1])\n",
    "offsetTime = Time(offsets.index[0],scale='tai')\n",
    "commandedAzShift = offsets.values[0][0]\n",
    "commandedElShift = offsets.values[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the mount data\n",
    "mount_position = await client.select_time_series(\"lsst.sal.ATMCS.mount_AzEl_Encoders\", ['*'],\n",
    "                                          exp1Start, exp2End)\n",
    "az = merge_packed_time_series(mount_position, 'azimuthCalculatedAngle', stride=1)\n",
    "el = merge_packed_time_series(mount_position, 'elevationCalculatedAngle', stride=1)\n",
    "print(len(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the tracking shift\n",
    "\n",
    "az_vals = np.array(az.values.tolist())[:,0]\n",
    "el_vals = np.array(el.values.tolist())[:,0]\n",
    "times = np.array(az.values.tolist())[:,1]\n",
    "\n",
    "shiftIndex = np.where(times>offsetTime.tai.unix)[0][0]\n",
    "valsPerSecond = 100\n",
    "n1 = np.where(times>exp1Start.tai.unix)[0][0] # Start of fist exposure\n",
    "n2 = np.where(times>exp1End.tai.unix)[0][0] # End of first exposure\n",
    "n3 = np.where(times>exp2Start.tai.unix)[0][0] # Start of second exposure\n",
    "try:\n",
    "    n4 = np.where(times>exp2End.tai.unix)[0][0] # End of second exposure\n",
    "except:\n",
    "    n4 = len(times)\n",
    "            \n",
    "# Fit the tracking before the offset with a quadratic\n",
    "az_vals_fit = az_vals[n1:n2]\n",
    "el_vals_fit = el_vals[n1:n2]\n",
    "times_fit = times[n1:n2]\n",
    "\n",
    "az_fit = np.polyfit(times_fit, az_vals_fit, 2)\n",
    "el_fit = np.polyfit(times_fit, el_vals_fit, 2)\n",
    "\n",
    "az_model = az_fit[0] * times * times + az_fit[1] * times + az_fit[2]\n",
    "el_model = el_fit[0] * times * times + el_fit[1] * times + el_fit[2]\n",
    "\n",
    "# Apply this model and calculate the departure from the model after the shift\n",
    "az_error = (az_vals - az_model) * 3600\n",
    "el_error = (el_vals - el_model) * 3600\n",
    "\n",
    "az_shift = np.mean(az_error[n3:n4])\n",
    "el_shift = np.mean(el_error[n3:n4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the shift\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "plt.subplot(3,1,1)\n",
    "plt.title(f\"Commands - 11-Mar-21\", fontsize = 18)\n",
    "plt.plot(times, times*0.0-1.0, color='green')\n",
    "plt.plot([exp1Start.tai.unix, exp1End.tai.unix], [0.1,0.1], color='red', lw=2.0, label=f\"exp:{expId1}\")\n",
    "plt.plot([exp2Start.tai.unix, exp2End.tai.unix], [0.1,0.1], color='red', lw=2.0, label=f\"exp:{expId2}\")\n",
    "plt.plot([offsetTime.tai.unix,offsetTime.tai.unix], [0.1, 0.4], color='blue', lw=2.0, label=\"Commanded Offset\")\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.legend()\n",
    "plt.subplot(3,1,2)\n",
    "plt.title(f\"Azimuth change\", fontsize = 18)\n",
    "plt.plot(times, az_error, color='green')\n",
    "plt.text(np.mean(times), np.mean(az_error), f\"az shift = {az_shift:.2f} arcsec\")\n",
    "plt.text(np.mean(times), np.mean(az_error)*0.5, f\"commanded az shift = {commandedAzShift:.1f} arcsec\")\n",
    "plt.ylabel(\"Arcsec\")\n",
    "plt.subplot(3,1,3)\n",
    "plt.title(f\"Elevation change\", fontsize = 18)\n",
    "plt.plot(times, el_error, color='green')\n",
    "plt.text(np.mean(times), np.mean(el_error), f\"el shift = {el_shift:.2f} arcsec\")\n",
    "plt.text(np.mean(times), np.mean(el_error)*0.5, f\"commanded el shift = {commandedElShift:.1f} arcsec\")\n",
    "plt.ylabel(\"Arcsec\")\n",
    "plt.xlabel(\"Time (Unix - seconds)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw quickLook data.  Only Gen2 works for now\n",
    "REPO_DIR = '/project/shared/auxTel/rerun/quickLook'\n",
    "gen2_butler = gen2Butler(REPO_DIR)\n",
    "dayObs = '2021-03-11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the objects\n",
    "charVisits = {}\n",
    "for expId in [expId1, expId2]:\n",
    "    charVisit = {}\n",
    "    exp = gen2_butler.get('quickLookExp', detector=0, expId=expId)\n",
    "    charResult = charTask.run(exp)\n",
    "    sourceCatalog = charResult.sourceCat\n",
    "    maxFlux = np.nanmax(sourceCatalog['base_CircularApertureFlux_3_0_instFlux'])\n",
    "    selectBrightestSource = sourceCatalog['base_CircularApertureFlux_3_0_instFlux'] > maxFlux * 0.99\n",
    "    brightestSource = sourceCatalog.subset(selectBrightestSource)\n",
    "    brightestCentroid = (brightestSource['base_SdssCentroid_x'][0], \\\n",
    "                         brightestSource['base_SdssCentroid_y'][0])\n",
    "    brightCatalog = sourceCatalog.subset(sourceCatalog['base_CircularApertureFlux_3_0_instFlux'] > maxFlux * 0.001)\n",
    "    print(f\"expId:{expId}. Found {len(sourceCatalog)} sources, {len(brightCatalog)} bright sources\")\n",
    "    print(f\"Brightest centroid at {brightestCentroid}\")\n",
    "    charVisit['exp'] = exp\n",
    "    charVisit['brightestCentroid'] = brightestCentroid\n",
    "    charVisit['brightCatalog'] = brightCatalog\n",
    "    charVisits[f\"{expId}\"] = charVisit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = charVisits[f\"{expId2}\"]['brightestCentroid'][0] - charVisits[f\"{expId1}\"]['brightestCentroid'][0]\n",
    "dy = charVisits[f\"{expId2}\"]['brightestCentroid'][1] - charVisits[f\"{expId1}\"]['brightestCentroid'][1]\n",
    "dx_arcsec = dx * charVisits[f\"{expId2}\"]['exp'].getWcs().getPixelScale().asArcseconds()\n",
    "dy_arcsec = dy * charVisits[f\"{expId2}\"]['exp'].getWcs().getPixelScale().asArcseconds()\n",
    "tot_arcsec = np.sqrt(dx_arcsec**2 + dy_arcsec**2)\n",
    "print(f\"Pixel shifts: DeltaX = {dx:.1f}, DeltaY = {dy:.1f}\")\n",
    "print(f\"Angular shifts: DeltaX = {dx_arcsec:.2f}, DeltaY = {dy_arcsec:.2f} , Total = {tot_arcsec:.2f} arcsec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the images\n",
    "plt.figure(figsize=(16,8))\n",
    "plotCounter = 1\n",
    "for expId in [expId1, expId2]:\n",
    "    charVisit = charVisits[f\"{expId}\"]\n",
    "    plt.subplot(1,2,plotCounter)\n",
    "    plotCounter += 1\n",
    "    plt.title(f\"Image - {expId}\",fontsize=18)\n",
    "    arr = charVisit['exp'].image.array\n",
    "    arr = np.clip(arr, 1, 100000) # This image has some negative values, and this removes them\n",
    "    img = plt.imshow(arr, norm=LogNorm(vmin=1, vmax=1000),  interpolation='Nearest', cmap='gray')\n",
    "    cat = charVisit['brightCatalog']\n",
    "    plt.scatter(cat['base_SdssCentroid_x'],cat['base_SdssCentroid_y']\\\n",
    "                ,color='red', marker='x')\n",
    "    plt.scatter([charVisit['brightestCentroid'][0]],[charVisit['brightestCentroid'][1]] \\\n",
    "                ,color='green', marker='+', s=200)\n",
    "    colorbar(img)\n",
    "    plt.legend()\n",
    "    #plt.ylim(0,4000)\n",
    "plt.tight_layout(h_pad=1)\n",
    "#plt.savefig(f\"/project/cslage/AuxTel/offsets/Offsets_{expIds[0]}_{expIds[1]}_{expIds[2]}_{expIds[3]}_16Apr21.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
