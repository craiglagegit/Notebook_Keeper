{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AuxTel AzEl offsets - 15-Apr-21\n",
    "\n",
    "In this notebook, investigate az-el offsets from 11-Mar-21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time, os, asyncio, glob\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import astropy.io.fits as pf\n",
    "from astropy.time import Time, TimeDelta\n",
    "from astropy.time import Time, TimeDelta\n",
    "from astropy.coordinates import SkyCoord, AltAz, ICRS, EarthLocation, Angle, FK5\n",
    "import astropy.units as u\n",
    "\n",
    "from lsst.daf.butler import Butler as gen3Butler\n",
    "from lsst.daf.persistence import Butler as gen2Butler\n",
    "from lsst_efd_client import EfdClient\n",
    "from lsst.pipe.tasks.characterizeImage import CharacterizeImageTask, CharacterizeImageConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Gen3 butler\n",
    "dayObs = 20210311\n",
    "myExpIds = [2021031100259, 2021031100260, 2021031100261, 2021031100281, 2021031100282, 2021031100283]\n",
    "REPO_DIR = '/repo/main'\n",
    "butler = gen3Butler(REPO_DIR, collections=\"LATISS/raw/all\")\n",
    "\n",
    "exposureList = []\n",
    "for record in butler.registry.queryDimensionRecords(\"exposure\", where=\"exposure.day_obs=%d\"%dayObs):\n",
    "    exposureList.append(record.id)\n",
    "exposureList.sort()\n",
    "myVisits = []\n",
    "for exposure in exposureList:\n",
    "    mData = butler.get('raw.metadata', detector=0, exposure=exposure)\n",
    "    expTime = mData['EXPTIME']\n",
    "    imgType = mData['IMGTYPE']\n",
    "    obj = mData['OBJECT']\n",
    "    filter = mData['FILTER']\n",
    "    rotpa = mData['ROTPA']\n",
    "    date_beg = mData['DATE-BEG']\n",
    "    elstart = mData['ELSTART']\n",
    "    azstart = mData['AZSTART']\n",
    "    rastart = mData['RASTART']\n",
    "    decstart = mData['DECSTART']\n",
    "    dummy=0.0\n",
    "    # Need to use DATE-BEG to get the right timestamp\n",
    "    visit = (exposure, expTime, imgType, obj, filter, date_beg, rotpa, dummy, dummy, azstart, elstart, rastart, decstart)\n",
    "    if (int(visit[0]) in myExpIds):\n",
    "        myVisits.append(visit)\n",
    "        print(visit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "mData = butler.get('raw.metadata', detector=0, exposure=2021031100261)\n",
    "for key in mData.keys():\n",
    "    print(key, mData[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for visit in myVisits:\n",
    "    print(visit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get EFD client\n",
    "client = EfdClient('summit_efd')\n",
    "\n",
    "def merge_packed_time_series(packed_dataframe, base_field, stride=1, \n",
    "                             ref_timestamp_col=\"cRIO_timestamp\", internal_time_scale=\"tai\"):\n",
    "    \"\"\"Select fields that are time samples and unpack them into a dataframe.\n",
    "            Parameters\n",
    "            ----------\n",
    "            packedDF : `pandas.DataFrame`\n",
    "                packed data frame containing the desired data\n",
    "            base_field :  `str`\n",
    "                Base field name that will be expanded to query all\n",
    "                vector entries.\n",
    "            stride : `int`, optional\n",
    "                Only use every stride value when unpacking.  Must be a factor\n",
    "                of the number of packed values.\n",
    "                (1 by default)\n",
    "            ref_timestamp_col : `str`, optional\n",
    "                Name of the field name to use to assign timestamps to unpacked\n",
    "                vector fields (default is 'cRIO_timestamp').\n",
    "            internal_time_scale : `str`, optional\n",
    "                Time scale to use when converting times to internal formats\n",
    "                ('tai' by default). Equivalent to EfdClient.internal_scale\n",
    "        Returns\n",
    "            -------\n",
    "            result : `pandas.DataFrame`\n",
    "                A `pandas.DataFrame` containing the results of the query.\n",
    "            \"\"\"\n",
    "    \n",
    "    packed_fields = [k for k in packed_dataframe.keys() if k.startswith(base_field)]\n",
    "    packed_fields = sorted(packed_fields, key=lambda k: int(k[len(base_field):]))  # sort by pack ID\n",
    "    npack = len(packed_fields)\n",
    "    if npack%stride != 0:\n",
    "        raise RuntimeError(f\"Stride must be a factor of the number of packed fields: {stride} v. {npack}\")\n",
    "    packed_len = len(packed_dataframe)\n",
    "    n_used = npack//stride   # number of raw fields being used\n",
    "    output = np.empty(n_used*packed_len)\n",
    "    times = np.empty_like(output, dtype=packed_dataframe[ref_timestamp_col][0])\n",
    "    \n",
    "    if packed_len == 1:\n",
    "        dt = 0\n",
    "    else:\n",
    "        dt = (packed_dataframe[ref_timestamp_col][1] - packed_dataframe[ref_timestamp_col][0])/npack\n",
    "    for i in range(0, npack, stride):\n",
    "        i0 = i//stride\n",
    "        output[i0::n_used] = packed_dataframe[f\"{base_field}{i}\"]\n",
    "        times[i0::n_used] = packed_dataframe[ref_timestamp_col] + i*dt\n",
    "     \n",
    "    timestamps = Time(times, format='unix', scale=internal_time_scale).datetime64\n",
    "    return pd.DataFrame({base_field:output, \"times\":times}, index=timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are for finding the timestamps of the offset events\n",
    "backUp = 120 # seconds before first image to get initial offset\n",
    "start = Time(myVisits[0][5],scale='tai') - TimeDelta(backUp, format='sec')\n",
    "end = Time(myVisits[-1][5],scale='tai') - TimeDelta(0, format='sec')\n",
    "timestamp = f\"time >= {start} AND time <= {end}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the offsets applied and the nasmyth angle\n",
    "offsets = await client.select_time_series(\"lsst.sal.ATPtg.command_offsetAzEl\", ['*'],\n",
    "                                          start, end)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(offsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, offset in enumerate(offsets.values):\n",
    "    print(i, Time(offsets.index[i]).tai.isot,offset[0], offset[1])\n",
    "    if i > 8:\n",
    "        break           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "expId = 2021031100261\n",
    "mData = butler.get('raw.metadata', detector=0, exposure=expId)\n",
    "rotpa = mData['ROTPA']\n",
    "date_beg = mData['DATE-BEG']\n",
    "date_end = mData['DATE-END']\n",
    "el = mData['ELSTART']\n",
    "backup = 0.0\n",
    "start = Time(date_beg,scale='tai') - TimeDelta(backUp, format='sec')\n",
    "end = Time(date_end,scale='tai')\n",
    "nasmyth_position = await client.select_time_series(\"lsst.sal.ATMCS.mount_Nasmyth_Encoders\", ['*'],\n",
    "                                              start, end)\n",
    "\n",
    "rot = merge_packed_time_series(nasmyth_position, 'nasmyth2CalculatedAngle', stride=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rot.values[0][0], rot.values[-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bore_sight_angle = el - rot.values[0][0] + 90.0\n",
    "print(bore_sight_angle)\n",
    "#off = np.array([126.0, 55.0])\n",
    "off = np.array([140.0, 0.0])\n",
    "theta = Angle(bore_sight_angle*u.deg).rad \n",
    "c, s = np.cos(theta), np.sin(theta)\n",
    "R = np.array(((c, -s), (s, c))) \n",
    "rotated_off = R.dot(off)\n",
    "print(rotated_off)\n",
    "print(np.arctan2(12.5958, -139.432) * 180.0 / np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first few to check the interleaving of the offsets with the exposures\n",
    "# Blue are the times of setting the offsets, and red are the start of the exposure\n",
    "startPlot = Time('2021-03-12T02:28:10',scale='tai') \n",
    "endPlot = Time('2021-03-12T02:28:55',scale='tai')\n",
    "mount_position = await client.select_time_series(\"lsst.sal.ATMCS.mount_AzEl_Encoders\", ['*'],\n",
    "                                          startPlot, endPlot)\n",
    "time.sleep(2.0)\n",
    "az = merge_packed_time_series(mount_position, 'azimuthCalculatedAngle', stride=1)\n",
    "el = merge_packed_time_series(mount_position, 'elevationCalculatedAngle', stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(el.values.tolist()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the tracking errors\n",
    "az_vals = np.array(az.values.tolist())[:,0]\n",
    "el_vals = np.array(el.values.tolist())[:,0]\n",
    "times = np.array(az.values.tolist())[:,1]\n",
    "times = times - times [0]\n",
    "\n",
    "az_vals_fit = np.array(az.values.tolist())[0:2000,0]\n",
    "el_vals_fit = np.array(el.values.tolist())[0:2000,0]\n",
    "times_fit = np.array(az.values.tolist())[0:2000,1]\n",
    "times_fit = times_fit - times_fit [0]\n",
    "\n",
    "# Fit with a quadratic\n",
    "az_fit = np.polyfit(times_fit, az_vals_fit, 2)\n",
    "el_fit = np.polyfit(times_fit, el_vals_fit, 2)\n",
    "\n",
    "az_model = az_fit[0] * times * times + az_fit[1] * times + az_fit[2]\n",
    "el_model = el_fit[0] * times * times + el_fit[1] * times + el_fit[2]\n",
    "\n",
    "# Errors in arcseconds\n",
    "az_error = (az_vals - az_model) * 3600\n",
    "el_error = (el_vals - el_model) * 3600\n",
    "\n",
    "az_shift = np.mean(az_error[3200:4000])\n",
    "el_shift = np.mean(el_error[3200:4000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,16))\n",
    "plt.subplots_adjust(hspace=1.0)\n",
    "plt.subplot(3,1,1)\n",
    "plt.title(f\"Commands - 11-Mar-21\", fontsize = 18)\n",
    "# Azimuth axis\n",
    "ax1 = offsets['num'].plot(color='red')\n",
    "ax1.set_ylim(0,1.0)\n",
    "for i in range(6):\n",
    "    try:\n",
    "        t1 = Time(offsets.index[i]).tai.isot\n",
    "        ax1.axvline(t1, ymin=0.5, ymax=0.9, color=\"blue\")\n",
    "    except:\n",
    "        pass\n",
    "    t2 = Time(myVisits[i][5]).tai.isot\n",
    "    ax1.axvline(t2, ymin=0.1, ymax=0.5, color=\"red\")\n",
    "ax1.set_xlim(startPlot.tai.isot,endPlot.tai.isot)\n",
    "plt.subplot(3,1,2)\n",
    "plt.title(f\"Azimuth change\", fontsize = 18)\n",
    "plt.plot(times, az_error, color='green')\n",
    "plt.text(30.0,5.0, f\"AZ_shift = {az_shift:.4f} arcsec\")\n",
    "plt.subplot(3,1,3)\n",
    "plt.title(f\"Elevation change\", fontsize = 18)\n",
    "plt.plot(times, el_error, color='green')\n",
    "plt.text(30.0,5.0, f\"EL_shift = {el_shift:.4f} arcsec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now append the applied offsets to the list of visits\n",
    "# A few drop out because the offsets are not clear\n",
    "backUp = 240\n",
    "fullVisits = []\n",
    "for i, visit in enumerate(myVisits):\n",
    "    newList = list(visit)\n",
    "    if i == 0:\n",
    "        startTime = Time(myVisits[i][5],scale='tai',precision=0) - TimeDelta(backUp, format='sec')\n",
    "        startTime = startTime.tai.isot\n",
    "    elif i==3 or i==4:\n",
    "        startTime = Time(myVisits[1][5],scale='tai',precision=0).tai.isot\n",
    "    else:\n",
    "        startTime = Time(myVisits[i - 1][5],scale='tai',precision=0).tai.isot\n",
    "    endTime = Time(myVisits[i][5],scale='tai',precision=0).tai.isot\n",
    "    #print(startTime, endTime)\n",
    "    try:\n",
    "        offset = offsets.loc[startTime:endTime].values\n",
    "        if len(offset) == 1:\n",
    "\n",
    "            newList[7] = offset[0][0]\n",
    "            newList[8] = offset[0][1]\n",
    "            \n",
    "        else:\n",
    "            print(\"Not = 1\", len(offset))\n",
    "            pass\n",
    "    except:\n",
    "        print(\"Failed the try\")\n",
    "        pass\n",
    "    print(newList[0], newList[7], newList[8])\n",
    "    fullVisits.append(newList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fullVisit in fullVisits:\n",
    "    print(fullVisit[0],fullVisit[7],fullVisit[8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get the raw quickLook data.  Only Gen2 works\n",
    "REPO_DIR = '/project/shared/auxTel/rerun/quickLook'\n",
    "gen2_butler = gen2Butler(REPO_DIR)\n",
    "dayObs = '2021-03-11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charConfig = CharacterizeImageConfig()\n",
    "charConfig.doMeasurePsf = False#True\n",
    "charConfig.doApCorr = False\n",
    "charConfig.doDeblend = False\n",
    "charConfig.repair.doCosmicRay = True\n",
    "charConfig.repair.doInterpolate = True   \n",
    "charConfig.detection.minPixels = 500\n",
    "charTask = CharacterizeImageTask(config=charConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Try doing them all\n",
    "charVisits = []\n",
    "for fullVisit in fullVisits:\n",
    "    expId = fullVisit[0]\n",
    "    try:\n",
    "        charVisit = {}\n",
    "        charVisit['Visit'] = fullVisit\n",
    "        exp = gen2_butler.get('quickLookExp', detector=0, expId=expId)\n",
    "        charResult = charTask.run(exp)\n",
    "        sourceCatalog = charResult.sourceCat\n",
    "        #print(expId)\n",
    "        #print(sourceCatalog['base_SdssShape_instFlux'])\n",
    "        maxFlux = np.nanmax(sourceCatalog['base_CircularApertureFlux_3_0_instFlux'])\n",
    "        selectBrightestSource = sourceCatalog['base_CircularApertureFlux_3_0_instFlux'] > maxFlux * 0.99\n",
    "        brightestSource = sourceCatalog.subset(selectBrightestSource)\n",
    "        brightestCentroid = (brightestSource['base_SdssCentroid_x'][0], \\\n",
    "                             brightestSource['base_SdssCentroid_y'][0])\n",
    "        brightCatalog = sourceCatalog.subset(sourceCatalog['base_CircularApertureFlux_3_0_instFlux'] > maxFlux * 0.001)\n",
    "        print(f\"expId:{expId}. Found {len(sourceCatalog)} sources, {len(brightCatalog)} bright sources\")\n",
    "        print(f\"Brightest centroid at {brightestCentroid}\")\n",
    "        charVisit['exp'] = exp\n",
    "        charVisit['brightestCentroid'] = brightestCentroid\n",
    "        charVisit['brightCatalog'] = sourceCatalog#brightCatalog\n",
    "        charVisits.append(charVisit)\n",
    "    except:\n",
    "        print(f\"Skipping expId {expId}.\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for charVisit in charVisits:\n",
    "    print(charVisit['Visit'][0], charVisit['Visit'][7], charVisit['Visit'][8])\n",
    "    print(charVisit['brightestCentroid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open('/project/cslage/AuxTel/offsets/offsets_HD75519_27apr21.pkl','wb')\n",
    "\n",
    "pkl.dump(charVisits,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open('/project/cslage/AuxTel/offsets/offsets_HD75519_27apr21.pkl','rb')\n",
    "charVisits = pkl.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([charVisits[1]['brightestCentroid'][0] - charVisits[0]['brightestCentroid'][0], \\\n",
    "    charVisits[1]['brightestCentroid'][1] - charVisits[0]['brightestCentroid'][1]], \\\n",
    "      np.array([charVisits[1]['Visit'][7] - charVisits[0]['Visit'][7], \\\n",
    "    charVisits[1]['Visit'][8] - charVisits[0]['Visit'][8]]) / 0.095)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([charVisits[2]['brightestCentroid'][0] - charVisits[1]['brightestCentroid'][0], \\\n",
    "    charVisits[2]['brightestCentroid'][1] - charVisits[1]['brightestCentroid'][1]], \\\n",
    "      np.array([charVisits[2]['Visit'][7] - charVisits[1]['Visit'][7], \\\n",
    "    charVisits[2]['Visit'][8] - charVisits[1]['Visit'][8]]) / 0.095)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([charVisits[2]['brightestCentroid'][0] - charVisits[1]['brightestCentroid'][0], \\\n",
    "    charVisits[2]['brightestCentroid'][1] - charVisits[1]['brightestCentroid'][1]], \\\n",
    "      np.array([charVisits[2]['Visit'][7], \\\n",
    "    charVisits[2]['Visit'][8]]) / 0.095)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([charVisits[5]['brightestCentroid'][0] - charVisits[4]['brightestCentroid'][0], \\\n",
    "    charVisits[5]['brightestCentroid'][1] - charVisits[4]['brightestCentroid'][1]], \\\n",
    "      np.array([charVisits[5]['Visit'][7] - charVisits[4]['Visit'][7], \\\n",
    "    charVisits[5]['Visit'][8] - charVisits[4]['Visit'][8]]) / 0.095)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([charVisits[5]['brightestCentroid'][0] - charVisits[4]['brightestCentroid'][0], \\\n",
    "    charVisits[5]['brightestCentroid'][1] - charVisits[4]['brightestCentroid'][1]], \\\n",
    "      np.array([charVisits[5]['Visit'][7], \\\n",
    "    charVisits[5]['Visit'][8]]) / 0.095)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([charVisits[4]['brightestCentroid'][0] - charVisits[3]['brightestCentroid'][0], \\\n",
    "    charVisits[4]['brightestCentroid'][1] - charVisits[3]['brightestCentroid'][1]], \\\n",
    "      np.array([charVisits[4]['Visit'][7] - charVisits[3]['Visit'][7], \\\n",
    "    charVisits[4]['Visit'][8] - charVisits[3]['Visit'][8]]) / 0.095)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "# Look at the data with matplotlib\n",
    "def colorbar(mappable):\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    last_axes = plt.gca()\n",
    "    ax = mappable.axes\n",
    "    fig = ax.figure\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = fig.colorbar(mappable, cax=cax)\n",
    "    plt.sca(last_axes)\n",
    "    return cbar\n",
    "\n",
    "\n",
    "#Plot these four\n",
    "expIds = [2021031100260, 2021031100261, 2021031100282, 2021031100283]\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plotCounter = 1\n",
    "for charVisit in charVisits:\n",
    "    expId = charVisit['Visit'][0]\n",
    "    offset = (charVisit['Visit'][7],charVisit['Visit'][8])\n",
    "    if expId not in expIds:\n",
    "        continue\n",
    "    plt.subplot(2,2,plotCounter)\n",
    "    plotCounter += 1\n",
    "    plt.title(f\"Image - {expId}\",fontsize=18)\n",
    "    arr = charVisit['exp'].image.array\n",
    "    arr = np.clip(arr, 1, 100000) # This image has some negative values, and this removes them\n",
    "    img = plt.imshow(arr, norm=LogNorm(vmin=1, vmax=1000),  interpolation='Nearest', cmap='gray')\n",
    "    cat = charVisit['brightCatalog']\n",
    "    plt.scatter(cat['base_SdssCentroid_x'],cat['base_SdssCentroid_y']\\\n",
    "                ,color='red', marker='x', label=f\"offset={offset}\")\n",
    "    plt.scatter([charVisit['brightestCentroid'][0]],[charVisit['brightestCentroid'][1]] \\\n",
    "                ,color='green', marker='+', s=100)\n",
    "    colorbar(img)\n",
    "    plt.legend()\n",
    "    plt.ylim(0,4000)\n",
    "plt.tight_layout(h_pad=1)\n",
    "#plt.savefig(f\"/project/cslage/AuxTel/offsets/Offsets_{expIds[0]}_{expIds[1]}_{expIds[2]}_{expIds[3]}_16Apr21.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arctan2(12.5958,-139.43222)*180.0/np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first few to check the interleaving of the offsets with the exposures\n",
    "# Blue are the times of setting the offsets, and red are the start of the exposure\n",
    "startPlot = Time('2021-03-12T04:36:36',scale='tai') \n",
    "endPlot = Time('2021-03-12T04:37:00',scale='tai')\n",
    "mount_position = await client.select_time_series(\"lsst.sal.ATMCS.mount_AzEl_Encoders\", ['*'],\n",
    "                                          startPlot, endPlot)\n",
    "time.sleep(2.0)\n",
    "az = merge_packed_time_series(mount_position, 'azimuthCalculatedAngle', stride=1)\n",
    "el = merge_packed_time_series(mount_position, 'elevationCalculatedAngle', stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(el.values.tolist()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the tracking errors\n",
    "n1=0\n",
    "n2=1000\n",
    "n3=2000\n",
    "n4=2200\n",
    "\n",
    "\n",
    "az_vals = np.array(az.values.tolist())[:,0]\n",
    "el_vals = np.array(el.values.tolist())[:,0]\n",
    "times = np.array(az.values.tolist())[:,1]\n",
    "times = times# - times [0]\n",
    "\n",
    "az_vals_fit = np.array(az.values.tolist())[n1:n2,0]\n",
    "el_vals_fit = np.array(el.values.tolist())[n1:n2,0]\n",
    "times_fit = np.array(az.values.tolist())[n1:n2,1]\n",
    "times_fit = times_fit# - times_fit [0]\n",
    "\n",
    "# Fit with a quadratic\n",
    "az_fit = np.polyfit(times_fit, az_vals_fit, 2)\n",
    "el_fit = np.polyfit(times_fit, el_vals_fit, 2)\n",
    "\n",
    "az_model = az_fit[0] * times * times + az_fit[1] * times + az_fit[2]\n",
    "el_model = el_fit[0] * times * times + el_fit[1] * times + el_fit[2]\n",
    "\n",
    "# Errors in arcseconds\n",
    "az_error = (az_vals - az_model) * 3600\n",
    "el_error = (el_vals - el_model) * 3600\n",
    "\n",
    "az_shift = np.mean(az_error[n3:n4])\n",
    "el_shift = np.mean(el_error[n3:n4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,16))\n",
    "plt.subplots_adjust(hspace=1.0)\n",
    "plt.subplot(3,1,1)\n",
    "plt.title(f\"Commands - 11-Mar-21\", fontsize = 18)\n",
    "# Azimuth axis\n",
    "ax1 = offsets['num'].plot(color='red')\n",
    "ax1.set_ylim(0,1.0)\n",
    "for i in range(6):\n",
    "    try:\n",
    "        t1 = Time(offsets.index[i]).tai.isot\n",
    "        ax1.axvline(t1, ymin=0.5, ymax=0.9, color=\"blue\")\n",
    "    except:\n",
    "        pass\n",
    "    t2 = Time(myVisits[i][5]).tai.isot\n",
    "    ax1.axvline(t2, ymin=0.1, ymax=0.5, color=\"red\")\n",
    "ax1.set_xlim(startPlot.tai.isot,endPlot.tai.isot)\n",
    "plt.subplot(3,1,2)\n",
    "plt.title(f\"Azimuth change\", fontsize = 18)\n",
    "#ax2 = az['azimuthCalculatedAngle'].plot(legend=False, color='green')\n",
    "plt.plot(times, az_error, color='green')\n",
    "plt.text(times[n2], 20.0, f\"AZ_shift = {az_shift:.4f} arcsec\")\n",
    "plt.subplot(3,1,3)\n",
    "plt.title(f\"Elevation change\", fontsize = 18)\n",
    "#ax3 = el['elevationCalculatedAngle'].plot(legend=False, color='green')\n",
    "plt.plot(times, el_error, color='green')\n",
    "plt.text(times[n2], -80.0, f\"EL_shift = {el_shift:.4f} arcsec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
